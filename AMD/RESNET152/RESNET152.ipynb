{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf8767e",
   "metadata": {},
   "source": [
    "# RESNET-152 TESTING BASE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444075f5",
   "metadata": {},
   "source": [
    "Uncomment below code, on first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aff1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET-152 BENCHMARK –(AMD ROCm on Windows)\n",
    "# Install ROCm SDK + PyTorch ROCm wheels + utilities\n",
    "# Python 3.12 required\n",
    "\n",
    "\n",
    "# %pip install --upgrade pip\n",
    "\n",
    "# 1) ROCm environment packages (as per AMD docs)\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/rocm_sdk_core-0.1.dev0-py3-none-win_amd64.whl\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/rocm_sdk_devel-0.1.dev0-py3-none-win_amd64.whl\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/rocm_sdk_libraries_custom-0.1.dev0-py3-none-win_amd64.whl\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/rocm-0.1.dev0.tar.gz\n",
    "\n",
    "# 2) PyTorch ROCm wheels (torch / torchaudio / torchvision)\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/torch-2.9.0+rocmsdk20251116-cp312-cp312-win_amd64.whl\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/torchaudio-2.9.0+rocmsdk20251116-cp312-cp312-win_amd64.whl\n",
    "# %pip install --no-cache-dir https://repo.radeon.com/rocm/windows/rocm-rel-7.1.1/torchvision-0.24.0+rocmsdk20251116-cp312-cp312-win_amd64.whl\n",
    "\n",
    "# %pip install onnx onnxruntime-gpu\n",
    "# %pip install matplotlib pillow tqdm requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PYTORCH / ROCM DEVICE CHECK ===\n",
      "Torch version: 2.9.0+rocmsdk20251116\n",
      "Torchvision version: 0.24.0+rocmsdk20251116\n",
      "\n",
      "torch.cuda.is_available(): False\n",
      "No GPU detected. Training will run on CPU.\n",
      "Check that AMD ROCm 7.1.1 is correctly installed and GPU is supported.\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"=== PYTORCH / ROCM DEVICE CHECK ===\")\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print()\n",
    "\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"GPU device [0]:\", torch.cuda.get_device_name(0))\n",
    "    except Exception as e:\n",
    "        print(\"Error reading device name:\", e)\n",
    "\n",
    "    try:\n",
    "        print(\"Reported CUDA version:\", torch.version.cuda)\n",
    "    except:\n",
    "        print(\"torch.version.cuda unavailable\")\n",
    "\n",
    "    try:\n",
    "        from torch.utils.collect_env import get_env_info\n",
    "        print(\"\\n--- Environment Info (ROCm Relevant) ---\")\n",
    "        print(get_env_info())\n",
    "    except:\n",
    "        print(\"collect_env not available\")\n",
    "else:\n",
    "    print(\"No GPU detected. Training will run on CPU.\")\n",
    "    print(\"Check that AMD ROCm 7.1.1 is correctly installed and GPU is supported.\")\n",
    "\n",
    "# Final device assignment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\nUsing device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb98f51",
   "metadata": {},
   "source": [
    "### Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ce72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset already prepared — skipping download + extraction.\n",
      "Images folder:       dataset_oxford_pet\\images\n",
      "Annotations folder:  dataset_oxford_pet\\annotations\n",
      "\n",
      "Final paths:\n",
      "Images folder:       dataset_oxford_pet\\images\n",
      "Annotations folder:  dataset_oxford_pet\\annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\CONFERENCE PAPER\\AMD\\RESNET152\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Download + Extract Oxford-IIIT Pet dataset\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import tarfile\n",
    "\n",
    "DATASET_DIR = Path(\"dataset_oxford_pet\")\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "IMAGES_DIR = DATASET_DIR / \"images\"\n",
    "ANN_DIR = DATASET_DIR / \"annotations\"\n",
    "\n",
    "# Dataset URLs\n",
    "URLS = {\n",
    "    \"images\": \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\",\n",
    "    \"annotations\": \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\",\n",
    "}\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download file with a progress bar.\"\"\"\n",
    "    print(f\"→ Checking: {output_path.name}\")\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(output_path, \"wb\") as file, tqdm(\n",
    "        desc=f\"Downloading {output_path.name}\",\n",
    "        total=total,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                bar.update(file.write(chunk))\n",
    "\n",
    "\n",
    "def safe_extract_tar(tar_file, extract_to):\n",
    "    \"\"\"Safe extraction to prevent path traversal.\"\"\"\n",
    "    print(f\"→ Extracting {tar_file.name} ...\")\n",
    "\n",
    "    with tarfile.open(tar_file) as tar:\n",
    "        def is_within_directory(directory, target):\n",
    "            abs_dir = os.path.abspath(directory)\n",
    "            abs_target = os.path.abspath(target)\n",
    "            return os.path.commonpath([abs_dir]) == os.path.commonpath([abs_dir, abs_target])\n",
    "\n",
    "        for member in tar.getmembers():\n",
    "            target_path = os.path.join(extract_to, member.name)\n",
    "            if not is_within_directory(extract_to, target_path):\n",
    "                raise Exception(\"Blocked unsafe path in tar file.\")\n",
    "\n",
    "        tar.extractall(path=extract_to)\n",
    "\n",
    "    print(\" Extraction complete.\\n\")\n",
    "\n",
    "\n",
    "# DOWNLOAD + EXTRACT LOGIC\n",
    "\n",
    "if IMAGES_DIR.exists() and ANN_DIR.exists():\n",
    "    print(\"✓ Dataset already prepared — skipping download + extraction.\")\n",
    "    print(\"Images folder:      \", IMAGES_DIR)\n",
    "    print(\"Annotations folder: \", ANN_DIR)\n",
    "else:\n",
    "    print(\"Dataset not found — downloading and extracting...\\n\")\n",
    "\n",
    "    for name, url in URLS.items():\n",
    "        tar_path = DATASET_DIR / f\"{name}.tar.gz\"\n",
    "\n",
    "        # Download only if tar.gz missing\n",
    "        if not tar_path.exists():\n",
    "            download_file(url, tar_path)\n",
    "        else:\n",
    "            print(f\"→ {tar_path.name} already exists — skipping download.\")\n",
    "\n",
    "        # Extract only if directory missing\n",
    "        target = DATASET_DIR / name\n",
    "        if not target.exists():\n",
    "            safe_extract_tar(tar_path, DATASET_DIR)\n",
    "        else:\n",
    "            print(f\"→ {name}/ already extracted — skipping extraction.\")\n",
    "\n",
    "    print(\"\\n✓ Dataset download + extraction complete.\")\n",
    "\n",
    "print(\"\\nFinal paths:\")\n",
    "print(\"Images folder:      \", IMAGES_DIR)\n",
    "print(\"Annotations folder: \", ANN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 37\n",
      "Train samples: 3680\n",
      "Val samples:   3669\n",
      "Sample tensor shape: torch.Size([3, 420, 420])\n",
      "Sample label: 0 | class name: Abyssinian\n"
     ]
    }
   ],
   "source": [
    "# Transforms (420x420 letterbox) + Oxford-IIIT Pet datasets\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "DATASET_ROOT = Path(\".\")          # project root\n",
    "OFFICIAL_DIR = DATASET_ROOT / \"oxford-iiit-pet\"\n",
    "LEGACY_DIR   = DATASET_ROOT / \"dataset_oxford_pet\"\n",
    "\n",
    "if LEGACY_DIR.exists() and not OFFICIAL_DIR.exists():\n",
    "    print(f\"Renaming '{LEGACY_DIR}' -> '{OFFICIAL_DIR}' for torchvision compatibility...\")\n",
    "    LEGACY_DIR.rename(OFFICIAL_DIR)\n",
    "\n",
    "class LetterboxToSquare420:\n",
    "    def __init__(self, size=420, fill=0):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # img is a PIL Image (W, H)\n",
    "        w, h = img.size\n",
    "        max_side = max(w, h)\n",
    "        scale = self.size / max_side\n",
    "\n",
    "        new_w = int(round(w * scale))\n",
    "        new_h = int(round(h * scale))\n",
    "\n",
    "        # Resize while preserving aspect ratio\n",
    "        img = F.resize(img, (new_h, new_w))\n",
    "\n",
    "        # Compute symmetric padding to reach size x size\n",
    "        pad_left   = (self.size - new_w) // 2\n",
    "        pad_right  = self.size - new_w - pad_left\n",
    "        pad_top    = (self.size - new_h) // 2\n",
    "        pad_bottom = self.size - new_h - pad_top\n",
    "\n",
    "        img = F.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=self.fill)\n",
    "        return img\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "common_transform = T.Compose([\n",
    "    LetterboxToSquare420(size=420, fill=0),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "train_dataset = OxfordIIITPet(\n",
    "    root=DATASET_ROOT,\n",
    "    split=\"trainval\",\n",
    "    target_types=\"category\",\n",
    "    transform=common_transform,\n",
    "    download=False, \n",
    ")\n",
    "\n",
    "val_dataset = OxfordIIITPet(\n",
    "    root=DATASET_ROOT,\n",
    "    split=\"test\",\n",
    "    target_types=\"category\",\n",
    "    transform=common_transform,\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples:   {len(val_dataset)}\")\n",
    "\n",
    "x0, y0 = train_dataset[0]\n",
    "print(\"Sample tensor shape:\", x0.shape)  # expect: [3, 420, 420]\n",
    "print(\"Sample label:\", y0, \"| class name:\", train_dataset.classes[y0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3de52",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:   32\n",
      "Num workers:  0\n",
      "Pin memory:   True\n",
      "Train samples: 3680 | Val samples: 3669\n",
      "Train batch shape: torch.Size([32, 3, 420, 420])\n",
      "Labels shape:      torch.Size([32])\n",
      "Unique labels in this batch: tensor([ 1,  5,  6,  8,  9, 10, 11, 12, 14, 15, 16, 17, 19, 20, 24, 26, 27, 28,\n",
      "        29, 32, 34, 36])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE   = 32\n",
    "NUM_WORKERS  = 0   \n",
    "PIN_MEMORY   = (device.type == \"cuda\")\n",
    "\n",
    "print(f\"Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"Num workers:  {NUM_WORKERS}\")\n",
    "print(f\"Pin memory:   {PIN_MEMORY}\")\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "batch_imgs, batch_labels = next(iter(train_loader))\n",
    "print(\"Train batch shape:\", batch_imgs.shape)        # expect: [BATCH_SIZE, 3, 420, 420]\n",
    "print(\"Labels shape:     \", batch_labels.shape)\n",
    "print(\"Unique labels in this batch:\", batch_labels.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (from dataset): 37\n",
      "Model: ResNet-152\n",
      "Total parameters: 58,219,621\n",
      "Trainable parameters: 58,219,621\n",
      "Device: cuda\n",
      "CUDA device: AMD Radeon RX 9060 XT\n",
      "AMP enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_7712\\1171432411.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152\n",
    "\n",
    "print(\"Number of classes (from dataset):\", num_classes)\n",
    "\n",
    "model = resnet152(weights=None)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30  \n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=50,\n",
    "    gamma=0.1,\n",
    ")\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model: ResNet-152\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"AMP enabled:\", USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation functions defined (AMP-ready, train + val).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"epoch_throughput_imgs_per_sec\": [],  # avg throughput per epoch\n",
    "}\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, criterion, epoch_idx: int,\n",
    "                    scaler, use_amp: bool):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    Returns:\n",
    "        avg_loss (float)\n",
    "        avg_throughput (float) - images per second across this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    total_batches = 0\n",
    "    throughput_sum = 0.0 \n",
    "\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch + 1} - Train\", leave=False)\n",
    "\n",
    "    for inputs, targets in loop:\n",
    "        batch_size = inputs.size(0)\n",
    "        total_samples += batch_size\n",
    "        total_batches += 1\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t1 = time.time()\n",
    "        batch_time = max(t1 - t0, 1e-6)\n",
    "\n",
    "        throughput = batch_size / batch_time\n",
    "        throughput_sum += throughput\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"loss\": loss.item(),\n",
    "            \"thrpt\": f\"{throughput:.1f} img/s\"\n",
    "        })\n",
    "\n",
    "    avg_loss = running_loss / max(total_samples, 1)\n",
    "    avg_throughput = throughput_sum / max(total_batches, 1)\n",
    "\n",
    "    return avg_loss, avg_throughput\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device, criterion, epoch_idx: int, use_amp: bool):\n",
    "    \"\"\"\n",
    "    Validation loop.\n",
    "    Returns:\n",
    "        avg_val_loss (float)\n",
    "        val_accuracy (float in [0, 1])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch + 1} - Val\", leave=False)\n",
    "\n",
    "    for inputs, targets in loop:\n",
    "        batch_size = inputs.size(0)\n",
    "        total_samples += batch_size\n",
    "\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"val_loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    avg_val_loss = running_loss / max(total_samples, 1)\n",
    "    val_acc = correct / max(total_samples, 1)\n",
    "\n",
    "    return avg_val_loss, val_acc\n",
    "\n",
    "print(\"Training and validation functions defined (AMP-ready, train + val).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Device: cuda\n",
      "Epochs: 30\n",
      "Batch size: 32\n",
      "AMP enabled: True\n",
      "\n",
      "===== Epoch 1/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train:   0%|          | 0/115 [00:00<?, ?it/s]C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_7712\\2933130138.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1 - Val:   0%|          | 0/115 [00:00<?, ?it/s]                                         C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_7712\\2933130138.py:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 summary:\n",
      "  Train loss:       4.1797\n",
      "  Val loss:         3.7791\n",
      "  Val accuracy:     2.59%\n",
      "  Avg throughput:   45.0 images/sec\n",
      "\n",
      "===== Epoch 2/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 summary:\n",
      "  Train loss:       3.6626\n",
      "  Val loss:         5.0172\n",
      "  Val accuracy:     2.92%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 3/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 summary:\n",
      "  Train loss:       3.6393\n",
      "  Val loss:         5.9656\n",
      "  Val accuracy:     2.83%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 4/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 summary:\n",
      "  Train loss:       3.6097\n",
      "  Val loss:         3.7417\n",
      "  Val accuracy:     3.98%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 5/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 summary:\n",
      "  Train loss:       3.5885\n",
      "  Val loss:         3.7483\n",
      "  Val accuracy:     3.62%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 6/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 summary:\n",
      "  Train loss:       3.5587\n",
      "  Val loss:         3.8587\n",
      "  Val accuracy:     3.98%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 7/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 summary:\n",
      "  Train loss:       3.4974\n",
      "  Val loss:         4.0880\n",
      "  Val accuracy:     4.52%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 8/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 summary:\n",
      "  Train loss:       3.4437\n",
      "  Val loss:         3.5434\n",
      "  Val accuracy:     5.59%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 9/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 summary:\n",
      "  Train loss:       3.3973\n",
      "  Val loss:         3.8161\n",
      "  Val accuracy:     5.51%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 10/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 summary:\n",
      "  Train loss:       3.3406\n",
      "  Val loss:         3.9691\n",
      "  Val accuracy:     8.07%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 11/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 summary:\n",
      "  Train loss:       3.2761\n",
      "  Val loss:         3.6491\n",
      "  Val accuracy:     8.20%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 12/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 summary:\n",
      "  Train loss:       3.2266\n",
      "  Val loss:         5.1892\n",
      "  Val accuracy:     9.89%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 13/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 summary:\n",
      "  Train loss:       3.1897\n",
      "  Val loss:         3.5103\n",
      "  Val accuracy:     9.29%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 14/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 summary:\n",
      "  Train loss:       3.1197\n",
      "  Val loss:         3.6117\n",
      "  Val accuracy:     10.03%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 15/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 summary:\n",
      "  Train loss:       3.0883\n",
      "  Val loss:         3.9725\n",
      "  Val accuracy:     9.65%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 16/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 summary:\n",
      "  Train loss:       3.0058\n",
      "  Val loss:         3.2685\n",
      "  Val accuracy:     12.40%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 17/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 summary:\n",
      "  Train loss:       2.9336\n",
      "  Val loss:         nan\n",
      "  Val accuracy:     13.90%\n",
      "  Avg throughput:   45.6 images/sec\n",
      "\n",
      "===== Epoch 18/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 summary:\n",
      "  Train loss:       2.8819\n",
      "  Val loss:         nan\n",
      "  Val accuracy:     14.61%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 19/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 summary:\n",
      "  Train loss:       2.7497\n",
      "  Val loss:         nan\n",
      "  Val accuracy:     14.15%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 20/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 summary:\n",
      "  Train loss:       2.6866\n",
      "  Val loss:         4.1233\n",
      "  Val accuracy:     14.85%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 21/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 summary:\n",
      "  Train loss:       2.6058\n",
      "  Val loss:         2.9775\n",
      "  Val accuracy:     16.54%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 22/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 summary:\n",
      "  Train loss:       2.5058\n",
      "  Val loss:         3.7077\n",
      "  Val accuracy:     17.06%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 23/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 summary:\n",
      "  Train loss:       2.4364\n",
      "  Val loss:         nan\n",
      "  Val accuracy:     17.25%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 24/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 summary:\n",
      "  Train loss:       2.3334\n",
      "  Val loss:         nan\n",
      "  Val accuracy:     9.54%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 25/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 summary:\n",
      "  Train loss:       2.3566\n",
      "  Val loss:         3.7376\n",
      "  Val accuracy:     16.93%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 26/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 summary:\n",
      "  Train loss:       2.1814\n",
      "  Val loss:         3.7071\n",
      "  Val accuracy:     18.81%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 27/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 summary:\n",
      "  Train loss:       2.0415\n",
      "  Val loss:         4.1452\n",
      "  Val accuracy:     15.73%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 28/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 summary:\n",
      "  Train loss:       1.9715\n",
      "  Val loss:         3.0338\n",
      "  Val accuracy:     21.29%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 29/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 summary:\n",
      "  Train loss:       1.8411\n",
      "  Val loss:         3.6361\n",
      "  Val accuracy:     19.08%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Epoch 30/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 summary:\n",
      "  Train loss:       1.7824\n",
      "  Val loss:         3.3941\n",
      "  Val accuracy:     21.20%\n",
      "  Avg throughput:   45.5 images/sec\n",
      "\n",
      "===== Training complete =====\n",
      "Total training time: 75.97 minutes\n",
      "Peak VRAM usage:    10926.2 MB (10.670 GB)\n",
      "Best val accuracy:  21.29%\n",
      "Final val accuracy: 21.20%\n",
      "\n",
      "NOTE:\n",
      "- Record avg GPU Util, avg VRAM, and avg power from HWiNFO at mid-training.\n",
      "- Later, we can compute performance per watt = best_epoch_throughput / avg_power_W.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"AMP enabled: {USE_AMP}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "total_train_start = time.time()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch + 1}/{EPOCHS} =====\")\n",
    "\n",
    "    train_loss, avg_throughput = train_one_epoch(\n",
    "        model=model,\n",
    "        loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        epoch_idx=epoch,\n",
    "        scaler=scaler,\n",
    "        use_amp=USE_AMP,\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = validate(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        epoch_idx=epoch,\n",
    "        use_amp=USE_AMP,\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"epoch_throughput_imgs_per_sec\"].append(avg_throughput)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} summary:\")\n",
    "    print(f\"  Train loss:       {train_loss:.4f}\")\n",
    "    print(f\"  Val loss:         {val_loss:.4f}\")\n",
    "    print(f\"  Val accuracy:     {val_acc * 100:.2f}%\")\n",
    "    print(f\"  Avg throughput:   {avg_throughput:.1f} images/sec\")\n",
    "\n",
    "total_train_end = time.time()\n",
    "total_training_time_sec = total_train_end - total_train_start\n",
    "\n",
    "peak_vram_bytes = None\n",
    "peak_vram_mb = None\n",
    "peak_vram_gb = None\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    peak_vram_bytes = torch.cuda.max_memory_allocated()\n",
    "    peak_vram_mb = peak_vram_bytes / (1024 ** 2)\n",
    "    peak_vram_gb = peak_vram_bytes / (1024 ** 3)\n",
    "\n",
    "history[\"total_training_time_sec\"] = total_training_time_sec\n",
    "history[\"peak_vram_bytes\"] = peak_vram_bytes\n",
    "history[\"peak_vram_mb\"] = peak_vram_mb\n",
    "history[\"peak_vram_gb\"] = peak_vram_gb\n",
    "history[\"best_val_acc\"] = best_val_acc\n",
    "history[\"final_val_acc\"] = history[\"val_acc\"][-1] if history[\"val_acc\"] else None\n",
    "\n",
    "print(\"\\n===== Training complete =====\")\n",
    "print(f\"Total training time: {total_training_time_sec / 60:.2f} minutes\")\n",
    "\n",
    "if peak_vram_mb is not None:\n",
    "    print(f\"Peak VRAM usage:    {peak_vram_mb:.1f} MB ({peak_vram_gb:.3f} GB)\")\n",
    "\n",
    "print(f\"Best val accuracy:  {best_val_acc * 100:.2f}%\")\n",
    "print(f\"Final val accuracy: {history['final_val_acc'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nNOTE:\")\n",
    "print(\"- Record avg GPU Util, avg VRAM, and avg power from HWiNFO at mid-training.\")\n",
    "print(\"- Later, we can compute performance per watt = best_epoch_throughput / avg_power_W.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
